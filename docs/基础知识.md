# 什么叫做嵌入层

[代码](book-0to1buildLLM/02-chapter/04-word-embedding.py)
嵌入层(Embedding Layer)是深度学习中的一个重要组件,主要用于将离散的符号(如词语、类别)转换为连续的向量表示。

## 核心概念

嵌入层本质上是一个**查找表**(lookup table),它将每个离散的输入映射到一个固定维度的稠密向量。比如:

- 词 "猫" → [0.2, -0.5, 0.8, ...]
- 词 "狗" → [0.3, -0.4, 0.7, ...]

## 为什么需要嵌入层?

1. **降维**: 独热编码(one-hot)对于大词汇表会产生非常高维且稀疏的向量,嵌入层可以将其压缩到较低维度的稠密向量

2. **学习语义关系**: 通过训练,相似含义的词会获得相近的向量表示,比如"国王-男人+女人 ≈ 王后"

3. **参数效率**: 相比直接处理原始离散数据,嵌入向量更适合神经网络处理

## 典型应用

- **NLP**: Word2Vec、GloVe 等词嵌入
- **推荐系统**: 用户 ID、商品 ID 的嵌入
- **分类任务**: 类别特征的嵌入

## 简单示例

```python
import torch.nn as nn

# 词汇表大小1000,嵌入维度128
embedding = nn.Embedding(num_embeddings=1000, embedding_dim=128)

# 输入词的索引 [5, 23, 100]
input_ids = torch.LongTensor([5, 23, 100])

# 输出形状: [3, 128]
output = embedding(input_ids)
```

嵌入层的权重在训练过程中会不断优化,以更好地表示数据的语义特征。

# 位置编码

1. 自注意力机制与位置无关: 这是核心要点。标准的自注意力机制在计算时,如果交换输入序列的顺序,输出也会相应交换。自注意力本身不知道词元的绝对或相对位置
2. 需要注入位置信息: 正是因为自注意力与位置无关,所以需要通过位置编码(Positional Encoding)来告诉模型词元的顺序信息

没有位置编码的话,"我爱中国"和"中国爱我"对模型来说是完全相同的。
总结: Transformer 架构的关键设计考量——自注意力的排列不变性(permutation invariance)需要通过位置编码来打破。
位置嵌入分为：

1. 相对嵌入
2. 绝对位置嵌入

# [上下文向量](book-0to1buildLLM/02-chapter/06-self-attention.py)

在大语言模型中，上下文向量（Context Vector）是指经过注意力机制处理后，融合了上下文信息的向量表示。
## 产生过程
1. 初始状态：每个 token 有自己的 embedding 向量（比如 768 维）
2. 注意力计算：通过 Self-Attention 机制，每个 token 会"关注"序列中的其他 token
3. 加权融合：根据注意力权重，将其他 token 的信息融合进来
4. 得到上下文向量：最终得到的向量不再是孤立的，而是包含了上下文信息
